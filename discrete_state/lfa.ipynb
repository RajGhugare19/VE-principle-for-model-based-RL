{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-GE6bl7U18e"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import time\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdXS33oIU9Gt"
      },
      "source": [
        "class Catch():\n",
        "\n",
        "    def __init__(self, rows = 10, cols = 5): \n",
        "\n",
        "        self.max_steps = 100\n",
        "        self.n_steps = 0\n",
        "        self.n_actions = 3\n",
        "        self.n_states = 250\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.colors = {0: (0,0,0),           #blank(object id = 0)\n",
        "                        1: (0, 255, 0),      #ball(object id = 2)\n",
        "                        2: (255, 0, 0)       #agent(object id = 3)\n",
        "                        } \n",
        "        self.env = np.zeros([self.rows,self.cols])\n",
        "        self.place_goal()\n",
        "        self.place_agent()\n",
        "\n",
        "    def place_goal(self):\n",
        "        \n",
        "        self.gx = 0\n",
        "        self.gy = np.random.randint(0,5)\n",
        "        self.env[self.gx,self.gy] = 1\n",
        "    \n",
        "    def place_agent(self):\n",
        "        \n",
        "        self.ax = self.rows-1\n",
        "        self.ay = np.random.randint(0,5)\n",
        "        self.env[self.ax,self.ay] = 2\n",
        "    \n",
        "    def step(self, action):\n",
        "\n",
        "        terminal = False\n",
        "\n",
        "        if action == 0:\n",
        "            reward = self.move(y=1)          #EAST\n",
        "        elif action == 1:\n",
        "            reward = self.move(y=-1)         #WEST \n",
        "        elif action == 2:                \n",
        "            reward = self.move(y=0)          #NOTHING\n",
        "        \n",
        "        if self.n_steps >= 100:\n",
        "            terminal = True\n",
        "\n",
        "        return (self.ay*50+self.gx*5+self.gy),reward,terminal\n",
        "\n",
        "    def move(self,y):\n",
        "\n",
        "        reward = 0\n",
        "        terminal = False\n",
        "        next_y = self.ay + y\n",
        "\n",
        "        if next_y == -1 or next_y == self.cols:\n",
        "            pass\n",
        "        else:\n",
        "            self.clear_block(self.ax,self.ay)\n",
        "            self.ay = next_y\n",
        "            self.place_block(2,self.ax,self.ay) \n",
        "        \n",
        "        if(self.gx==self.rows-1):\n",
        "            #If the ball is at the bottom most row\n",
        "            self.clear_block(self.gx,self.gy)\n",
        "            self.place_goal()\n",
        "        else:\n",
        "            self.clear_block(self.gx,self.gy)\n",
        "            self.gx += 1\n",
        "            self.place_block(1,self.gx,self.gy) \n",
        "\n",
        "            if(self.gx==self.rows-1):\n",
        "                if(self.gy==self.ay):\n",
        "                    reward = 1\n",
        "            \n",
        "\n",
        "        self.n_steps += 1\n",
        "        return reward\n",
        "\n",
        "    def clear_block(self,x,y):\n",
        "        \n",
        "        self.env[x,y] = 0\n",
        "\n",
        "    def place_block(self,obj,x,y):\n",
        "        \n",
        "        self.env[x,y] = obj\n",
        "\n",
        "    def reset(self):\n",
        "        \n",
        "        self.n_steps = 0\n",
        "        self.env = np.zeros([self.rows,self.cols])\n",
        "        self.place_goal()\n",
        "        self.place_agent()\n",
        "        return (self.ay*50+self.gx*5+self.gy)\n",
        "    \n",
        "    def render(self, render_time=100):\n",
        "        #cv2.destroyAllWindows()\n",
        "        img = np.zeros((self.rows*31, self.cols*31, 3), dtype=np.uint8)\n",
        "\n",
        "        for i in range(self.rows):\n",
        "            for j in range(self.cols):\n",
        "                obj = self.env[i,j]\n",
        "                img = self.fill(img,i,j,obj)\n",
        "\n",
        "        img = Image.fromarray(img, 'RGB')\n",
        "        cv2.imshow(\"image\", np.array(img))\n",
        "        cv2.waitKey(render_time)\n",
        "\n",
        "    def fill(self, m, x, y, c, s=31):\n",
        "\n",
        "        t = (s-1)//2\n",
        "        x_t = (x)*s + t\n",
        "        y_t = (y)*s + t\n",
        "        m[x_t-t:x_t+t,y_t-t:y_t+t] = self.colors[c]\n",
        "        return m\n",
        "\n",
        "    def sample_action(self):\n",
        "\n",
        "        return np.random.randint(0, self.n_actions)\n",
        "    \n",
        "    def state_features(self):\n",
        "        state = np.zeros([250,4],dtype=np.float32)\n",
        "        for i in range(250):\n",
        "            state[i,0] = 9\n",
        "            state[i,1] = i//50\n",
        "            state[i,2] = ((i//5)%10)\n",
        "            state[i,3] = i%5\n",
        "        return state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPJlun3GU_HX"
      },
      "source": [
        "class Memory():\n",
        "    def __init__(self,env,size):\n",
        "        \n",
        "        self.size = size\n",
        "        self.env = env\n",
        "        self.state_memory = np.zeros([size],dtype = np.int)\n",
        "        self.next_state_memory = np.zeros([size],dtype = np.int)\n",
        "        self.action_memory = np.zeros(size,dtype=np.int)\n",
        "        self.reward_memory = np.zeros(size,dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(size,dtype=np.int8)\n",
        "        self.memory_count = 0\n",
        "    \n",
        "    def store(self,state,action,reward,next_state,terminal):\n",
        "        \n",
        "        index = self.memory_count%self.size\n",
        "\n",
        "        self.state_memory[index] = state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.next_state_memory[index] = next_state\n",
        "        self.terminal_memory[index] = terminal\n",
        "        self.memory_count+=1 \n",
        "    \n",
        "    def to_tensor(self):\n",
        "        self.state_memory = torch.tensor(self.state_memory).to(device)\n",
        "        self.next_state_memory = torch.tensor(self.next_state_memory).to(device)\n",
        "        self.reward_memory = torch.tensor(self.reward_memory).to(device)\n",
        "\n",
        "    def store_random(self):\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        for i in range(self.size):\n",
        "            action = np.random.randint(0,self.env.n_actions)\n",
        "            next_state,reward,done = self.env.step(action)\n",
        "            self.store(state,action,reward,next_state,done)\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                done = False\n",
        "            else:\n",
        "                state = next_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hExQk64fgjcT"
      },
      "source": [
        "def rank_constrained(F_D,F_K):\n",
        "    m = torch.nn.Softmax(dim=2)\n",
        "    P = torch.matmul(m(F_D),m(F_K))\n",
        "    return P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2p03ylRVB-I"
      },
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "learning_rate = 5e-3\n",
        "total_iters = 1000000\n",
        "gamma = 0.99\n",
        "mem_size = 1000000\n",
        "d = 50\n",
        "rank = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z6gEAWpVKpP"
      },
      "source": [
        "env = Catch()\n",
        "features = env.state_features()\n",
        "mem = Memory(env,mem_size)\n",
        "mem.store_random()\n",
        "data = features[mem.state_memory]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgrggLhwkjcv"
      },
      "source": [
        "km = KMeans(\n",
        "  n_clusters=d, init='k-means++',\n",
        "  n_init=10,tol=1e-04)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL6bhPpeBHXB",
        "outputId": "3e4c1a91-0153-4979-a4c2-97bd9026a682"
      },
      "source": [
        "km.fit(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=50, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JJ-7OvzKwdp"
      },
      "source": [
        "ph_index = km.predict(features)\n",
        "ph_feature = np.eye(d)[ph_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS4xpIpTOTma"
      },
      "source": [
        "#Estimating reward and transition matrix\n",
        "Reward = torch.zeros(env.n_states,env.n_actions)\n",
        "Transition = torch.zeros(env.n_actions,env.n_states,env.n_states)\n",
        "for i in range(1000000):\n",
        "    s,a,r,n_s = mem.state_memory[i],mem.action_memory[i],mem.reward_memory[i],mem.next_state_memory[i]\n",
        "    Reward[s,a] += r\n",
        "    Transition[a,s,n_s] += 1\n",
        "z = torch.unsqueeze(torch.sum(Transition,axis=2),dim=2)\n",
        "P_A = torch.divide(Transition,z) #unbiased transition matrix\n",
        "R_A = torch.divide(Reward,torch.sum(Transition,axis=2).T) #unbiased reward matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_ybLiU2gQqQ"
      },
      "source": [
        "v_basis = torch.tensor(ph_feature, device = device)\n",
        "P_A = P_A.to(device)\n",
        "F_D = (torch.rand(size=[env.n_actions,env.n_states,rank],device=device)*2 - 1).requires_grad_(True)\n",
        "F_K = (torch.rand(size=[env.n_actions,rank,env.n_states],device=device)*2 - 1).requires_grad_(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2uc1pdYzlXF"
      },
      "source": [
        "#MLE \n",
        "for i in range(total_iters):\n",
        "    P = rank_constrained(F_D,F_K)\n",
        "    loss = torch.sum(torch.multiply(-P_A,torch.log(P)))\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        F_D -= learning_rate * F_D.grad \n",
        "        F_K -= learning_rate * F_K.grad \n",
        "    F_D.grad = None\n",
        "    F_K.grad = None\n",
        "    if i%5000 == 0:\n",
        "        print(\"i = \", i)\n",
        "        print(\"loss = \", loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTJXlu1Fgand"
      },
      "source": [
        "#VE\n",
        "T = torch.matmul(P_A,v_basis.float())\n",
        "for i in range(total_iters):\n",
        "    P = rank_constrained(F_D,F_K)\n",
        "    loss = torch.sum(torch.linalg.norm(T-torch.matmul(P,v_basis.float()),dim=2)**2)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        F_D -= learning_rate * F_D.grad \n",
        "        F_K -= learning_rate * F_K.grad\n",
        "    F_D.grad = None\n",
        "    F_K.grad = None\n",
        "    if i%5000 == 0:\n",
        "        print(\"i = \", i)\n",
        "        print(\"loss = \", loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0j9EK7YPxNQ"
      },
      "source": [
        "EVAlUATE USING LSTD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jYzR84ggdpH"
      },
      "source": [
        "def collect_traj(size,policy,P_es,R_es):\n",
        "    state = np.zeros(size,dtype=np.int)\n",
        "    next_state = np.zeros(size,dtype=np.int)\n",
        "    reward = np.zeros(size,dtype=np.int)\n",
        "    action = np.zeros(size,dtype=np.int)\n",
        "    term = np.zeros(size,dtype=np.int)\n",
        "    s = env.reset()\n",
        "    \n",
        "    #Storing on-policy data\n",
        "    for j in range(size):\n",
        "        \n",
        "        a = policy[s]\n",
        "        n_s,r,t = env.step(a)\n",
        "        n_s_hat = np.random.choice(state_list, 1, p=P_es[a,s])\n",
        "        #n_s_hat = np.argmax(P_es[a,s])\n",
        "        r_hat = R_es[s,a]\n",
        "        state[j] = s\n",
        "        action[j] = a\n",
        "        next_state[j] = n_s_hat\n",
        "        reward[j] = r_hat\n",
        "        term[j] = t\n",
        "        \n",
        "        if t:\n",
        "            s = env.reset()\n",
        "        else:\n",
        "            s = n_s\n",
        "    return state,next_state,reward,action,term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPUAsFofkV5d"
      },
      "source": [
        "def value_iteration(env,t,r,gamma=0.99,epsilon=0.01):\n",
        "    v = np.zeros(env.n_states)\n",
        "    v_new = np.zeros(env.n_states)\n",
        "    t = np.array(t.to('cpu').detach())\n",
        "    r = np.array(r.to('cpu').detach())\n",
        "    while True:\n",
        "        for s in range(env.n_states):\n",
        "            v_temp = np.zeros(env.n_actions)\n",
        "            for a in range(env.n_actions):\n",
        "                v_temp[a] = r[s,a] + gamma*np.dot(t[a,s,:],v) \n",
        "            v_new[s] = np.max(v_temp)\n",
        "        if np.max(np.abs(v - v_new)) < epsilon:\n",
        "            break\n",
        "        v = np.copy(v_new)\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dMoVC7PkXP9"
      },
      "source": [
        "def policy_eval(policies,env,t,r,gamma,epsilon=0.01):\n",
        "    v_pi = []\n",
        "    t = np.array(t.to('cpu'))\n",
        "    r = np.array(r.to('cpu'))\n",
        "    for policy in policies:\n",
        "        v = np.zeros(env.n_states)\n",
        "        v_new = np.zeros(env.n_states)\n",
        "        while True:\n",
        "            for s in range(env.n_states):\n",
        "                a = int(policy[s])\n",
        "                v_new[s] = r[s,a] + gamma*np.dot(t[a,s,:],v)\n",
        "            if np.max(np.abs(v - v_new)) < epsilon:\n",
        "                break\n",
        "            v = np.copy(v_new)\n",
        "        v_pi.append(v)\n",
        "    return v_pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrlgMBF0kUOD"
      },
      "source": [
        "def LSTD(state,next_state,reward,action,term,feature,d,gamma=1):\n",
        "    A = np.eye(d)*0.001 \n",
        "    b = np.zeros(d)\n",
        "    for i in range(10000):\n",
        "        s = state[i]\n",
        "        a = action[i]\n",
        "        r = reward[i]\n",
        "        n_s = next_state[i]\n",
        "        t = term[i]\n",
        "        \n",
        "        x = feature[s]\n",
        "        xp = feature[n_s]  \n",
        "        \n",
        "        A += np.outer(x, (x - gamma*xp))\n",
        "        b += x * r\n",
        "    w = np.dot(np.linalg.pinv(A), b)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNoH4BKkYlW"
      },
      "source": [
        "#Approx Policy iteration with LSTD\n",
        "state_list = np.arange(env.n_states)\n",
        "policy = np.random.randint(0,env.n_actions,(env.n_states))\n",
        "P_est = P.detach().cpu().numpy()\n",
        "R_est = R_A.detach().cpu().numpy()\n",
        "for i in range(40): \n",
        "    s,ns,r,a,t = collect_traj(10000,policy,P_est,R_est)\n",
        "    w = LSTD(s,ns,r,a,t,ph_feature,50)\n",
        "    for j in range(env.n_states):\n",
        "        v_temp = np.zeros(env.n_actions)\n",
        "        for k in range(env.n_actions):\n",
        "            n_j = np.random.choice(state_list, 1, p=P_est[k,j])\n",
        "            v_temp[k] = R_est[j,k] + gamma*np.dot(ph_feature[n_j],w)\n",
        "        policy[j] = np.argmax(v_temp)\n",
        "    v_cur = policy_eval([policy],env,P_A,R_A,gamma=0.99)\n",
        "    print(np.mean(v_cur))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO6FnW7RPrYh"
      },
      "source": [
        "EVALUATE USING DDQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOL3uGIifhTI"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    \n",
        "    def __init__(self,learning_rate,d,n_actions,epsilon):\n",
        "        \n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.d = d\n",
        "        \n",
        "        self.linear = nn.Linear(d,n_actions,bias=False)\n",
        "        nn.init.normal_(self.linear.weight,0.0,1/np.sqrt(d))\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        q_value = self.linear(x)        \n",
        "        return q_value\n",
        "    \n",
        "    def action(self,x):\n",
        "        x = torch.tensor(x).to(device)\n",
        "        r = np.random.random()\n",
        "        if r<self.epsilon:\n",
        "            action = np.random.randint(0,self.n_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_val = self.forward(x.float())\n",
        "                action = torch.argmax(q_val).item()\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxmF4mSlqSO4"
      },
      "source": [
        "n_actions = env.n_actions\n",
        "\n",
        "DQN_lr = 1e-3\n",
        "n_actions = env.n_actions\n",
        "epsilon = 0.01\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "num_learn_steps = 1000000\n",
        "num_learn_freq = 4\n",
        "dqn_gamma = 0.99\n",
        "target_update = 2000\n",
        "P_es = P.detach().cpu().numpy()\n",
        "R_es = R_A.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCBJGZWpvwi",
        "outputId": "05010281-2201-4985-b775-640f3c99ad94"
      },
      "source": [
        "Q_model = DQN(DQN_lr,d,n_actions,epsilon).to(device)\n",
        "Q_model_target = DQN(DQN_lr,d,n_actions,epsilon).to(device)\n",
        "Q_model_target.load_state_dict(Q_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vgJ2eEhqOdd"
      },
      "source": [
        "replay_buffer = Memory(env,10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1Z9vGIA1qhlB"
      },
      "source": [
        "batch_index = np.arange(batch_size)\n",
        "done = False\n",
        "s = env.reset()\n",
        "score = 0\n",
        "scores = []\n",
        "best_avg = 0\n",
        "for n in range(1,num_learn_steps+1):\n",
        "    a = Q_model.action(ph_feature[s])\n",
        "    ns,r,done = env.step(a)\n",
        "    score += r\n",
        "    ns_hat = np.random.choice(state_list, 1, p=P_es[a,s])\n",
        "    r_hat = R_es[s,a]\n",
        "\n",
        "    replay_buffer.store(s,a,r_hat,ns_hat,done)\n",
        "    if done:\n",
        "      scores.append(score)\n",
        "      s = env.reset()\n",
        "      done = False\n",
        "      score = 0\n",
        "    else:\n",
        "      s = ns\n",
        "    \n",
        "    if replay_buffer.memory_count < mem_size:\n",
        "        m = replay_buffer.memory_count\n",
        "    else:\n",
        "        m = mem_size\n",
        "\n",
        "    if replay_buffer.memory_count > batch_size and n%num_learn_freq==0 :\n",
        "\n",
        "        batch = np.random.choice(10000, batch_size, replace=False)\n",
        "        s_batch = torch.tensor(ph_feature[replay_buffer.state_memory[batch]]).to(device)\n",
        "        ns_batch = torch.tensor(ph_feature[replay_buffer.next_state_memory[batch]]).to(device)\n",
        "        r_batch = torch.tensor(replay_buffer.reward_memory[batch],dtype=torch.float32).to(device)\n",
        "        a_batch = torch.tensor(replay_buffer.action_memory[batch])\n",
        "        term_batch = torch.tensor(replay_buffer.terminal_memory[batch],dtype=torch.int8).to(device)\n",
        "        \n",
        "        q_val = Q_model.forward(s_batch.float())[batch_index,a_batch]\n",
        "        max_a = torch.argmax(Q_model.forward(ns_batch.float()),dim=1).detach()\n",
        "        next_q_val = Q_model_target(ns_batch.float())[batch_index,max_a].detach()\n",
        "\n",
        "        q_target = r_batch + dqn_gamma*next_q_val*(1-term_batch) \n",
        "        loss = Q_model.criterion(q_val,q_target).to(device)\n",
        "        \n",
        "        Q_model.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        Q_model.optimizer.step()\n",
        "        Q_model.optimizer.zero_grad()\n",
        "    \n",
        "    if n%500 == 0:\n",
        "      print(\"i = \", n)\n",
        "      print('score now', np.mean(scores))\n",
        "\n",
        "    if n%target_update == 0:\n",
        "        Q_model_target.load_state_dict(Q_model.state_dict())\n",
        "\n",
        "    if len(scores) == 100:\n",
        "        avg = np.mean(scores)\n",
        "        if avg > best_avg:\n",
        "            best_avg = avg\n",
        "            torch.save(Q_model.state_dict(),'best.pt')\n",
        "        scores.pop(0)\n",
        "best_avg "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efmovN_7DhWD",
        "outputId": "19e8d848-5dd3-4853-9daf-90bea390c77c"
      },
      "source": [
        "Q_model.load_state_dict(torch.load('best.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTn3HRaCrtYP"
      },
      "source": [
        "done = False\n",
        "score = 0\n",
        "scores = []\n",
        "\n",
        "for i in range(100): \n",
        "  s = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "  while not done:\n",
        "    s_ = torch.tensor(ph_feature[s]).to(device)\n",
        "    q_val = Q_model(s_.float())\n",
        "    a = torch.argmax(q_val).item()\n",
        "    ns,r,done = env.step(a)\n",
        "    score += r\n",
        "    s = ns\n",
        "  scores.append(score)\n",
        "print(np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu84XKQN0A7B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}